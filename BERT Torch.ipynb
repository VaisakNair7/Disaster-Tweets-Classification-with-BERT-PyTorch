{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98262a2",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46f5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "import functools\n",
    "from cachetools import cached, TTLCache\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37926bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f2bdb",
   "metadata": {},
   "source": [
    "### Read the train and test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89fd693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cc4b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1afce5",
   "metadata": {},
   "source": [
    "### Split df_train into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd39f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_split, test_split = train_test_split(df_train[['text', 'target']], test_size = 0.25, stratify = df_train['target'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db88bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split.reset_index(inplace = True)\n",
    "test_split.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea07c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8767, 1.1637])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the class weigths since its an imbalanced dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = [0, 1], y = df_train['target'])\n",
    "class_weights = torch.tensor(class_weights, dtype = torch.float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeadd3b",
   "metadata": {},
   "source": [
    "### Load 'bert-base-uncased' model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e401bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0adbb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7867,  3764,  2015, 16360,  2005,  1996,  9680,  6591,  1012,\n",
      "         11693,  2000, 11234,  1996,  2060,  2305,  2043,  7171,  8369,  2041,\n",
      "          2012, 28076,  2347,  1005,  1056,  1037,  2204,  4135,  6559,  1012,\n",
      "          2770,  1999,  6634,  2053,  4569,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Output of tokenizer - 'input_ids', 'token_type_ids' and 'attention_mask'\n",
    "text = train_split['text'][0]\n",
    "text = tokenizer(text, padding = 'max_length', max_length = 100, truncation = True, return_tensors = 'pt')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4558318",
   "metadata": {},
   "source": [
    "### Create PyTorch dataset to return the model inputs and targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a719eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = TTLCache(maxsize = 8000, ttl = 86400)\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    @cached(cache)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['text'][idx].lower()\n",
    "        #text = ' '.join(text.split())\n",
    "        target = self.df['target'][idx]\n",
    "        \n",
    "        text = tokenizer(text, padding = 'max_length', max_length = 100, truncation = True, return_tensors = 'pt')\n",
    "        ids = text['input_ids'].squeeze(0)\n",
    "        token_type_ids = text['token_type_ids'].squeeze(0)\n",
    "        masks = text['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'ids': ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'mask': masks,\n",
    "            'targets': torch.tensor(target, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb381e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and val datasets\n",
    "train_set = TwitterDataset(train_split, tokenizer)\n",
    "test_set = TwitterDataset(test_split, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661c777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and val dataloaders\n",
    "train_dataloader = DataLoader(train_set, batch_size = 8, shuffle = True, num_workers = 0)\n",
    "val_dataloader = DataLoader(test_set, batch_size = 8, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fcb71",
   "metadata": {},
   "source": [
    "### Create BERT model class by subclassing nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba6706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, ids, token_type_ids, masks):\n",
    "        out = self.model(ids, token_type_ids, masks)[1]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f524ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = BERTModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7420d9",
   "metadata": {},
   "source": [
    "### Loss, Optimizer and Learning rate scheduler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b987f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight = class_weights.to(device))\n",
    "\n",
    "optimizer = optim.AdamW(BERT.parameters(), lr = 3e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = 3, mode = 'min', verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87e814",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c3a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train on train split and validate on val split\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "\n",
    "        train_loss, val_loss = [], []\n",
    "        y_preds, y_true = [], []\n",
    "\n",
    "        # Train\n",
    "        train_loop = tqdm(train_dataloader, total = len(train_dataloader), position = 0, leave = True)\n",
    "\n",
    "        model.train()\n",
    "        for data in train_loop:\n",
    "            ids = data['ids'].to(device)\n",
    "            masks = data['mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(ids, token_type_ids, masks)\n",
    "            preds = torch.argmax(output, dim = 1).tolist()\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            y_preds.extend(preds)\n",
    "            y_true.extend(targets.detach().cpu().tolist())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loop.set_postfix(train_loss = np.mean(train_loss), train_f1 = f1_score(y_preds, y_true))\n",
    "\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loop = tqdm(val_dataloader, total = len(val_dataloader), position = 0, leave = True)\n",
    "\n",
    "        y_preds, y_true = [], []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loop:\n",
    "                ids = data['ids'].to(device)\n",
    "                masks = data['mask'].to(device)\n",
    "                token_type_ids = data['token_type_ids'].to(device)\n",
    "                targets = data['targets'].to(device)\n",
    "\n",
    "                output = model(ids, token_type_ids, masks)\n",
    "                preds = torch.argmax(output, dim = 1).tolist()\n",
    "                loss = criterion(output, targets)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "                y_preds.extend(preds)\n",
    "                y_true.extend(targets.detach().cpu().tolist())\n",
    "\n",
    "                val_loop.set_postfix(val_loss = np.mean(val_loss), val_f1 = f1_score(y_preds, y_true))\n",
    "\n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_f1 = f1_score(y_preds, y_true)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Save weights of model having best validation metric\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print('------------------------------------------------------------------\\n')\n",
    "\n",
    "    print(f'Training complete, Best F1 score : {best_f1 * 100:.2f}%')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67789ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 714/714 [05:49<00:00,  2.04it/s, train_f1=0.757, train_loss=0.457]\n",
      "100%|███████████████████████████████████████████████████| 238/238 [00:35<00:00,  6.62it/s, val_f1=0.82, val_loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 714/714 [05:48<00:00,  2.05it/s, train_f1=0.847, train_loss=0.328]\n",
      "100%|██████████████████████████████████████████████████| 238/238 [00:34<00:00,  6.89it/s, val_f1=0.804, val_loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 714/714 [05:48<00:00,  2.05it/s, train_f1=0.91, train_loss=0.217]\n",
      "100%|██████████████████████████████████████████████████| 238/238 [00:35<00:00,  6.79it/s, val_f1=0.805, val_loss=0.485]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "\n",
      "Training complete, Best F1 score : 81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# With 3 epochs we get highest F1-score of 81.96% on validation dataset\n",
    "model, train_losses, val_losses = train_model(BERT.to(device), criterion, optimizer, scheduler, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cc3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74547036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed7707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf63eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf534d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
